{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys, math, json, random\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "sys.path.insert(0, \"/home/joel/Repos/dlib_facedetector_pytorch\")\n",
    "import numpy as np\n",
    "import kornia\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from tqdm.autonotebook import tqdm\n",
    "from pytorch_pretrained_biggan import BigGAN, convert_to_images, save_as_images\n",
    "from src.notebook_utils import imshow, imgrid, pltshow, draw_tensors\n",
    "from src.face_loss import DlibFaceLoss\n",
    "from src.pytorch_utils import augment\n",
    "from src.palette import random_biggan, load_directory, load_images\n",
    "from src.collage import Collager\n",
    "from src.collage_save import CollageSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_loss = DlibFaceLoss(filter_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.gan import Generator\n",
    "mask_generator = Generator(img_size=128, latent_size=100, channels=1).cuda()\n",
    "# https://drive.google.com/file/d/1IhoB6lxbKxL66F0X99ntL-t3-XKnxDPZ/view?usp=sharing\n",
    "model_path = './saved_models/dcgan_gen_128'\n",
    "mask_generator.load_state_dict(torch.load(model_path))\n",
    "mask_generator.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "masks = mask_generator(torch.randn(10, 100).cuda())\n",
    "pltshow(np.hstack(masks[:, 0].detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make or load the palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sets = [\n",
    "    [ \n",
    "        './datasets/ab_biggan/2db513d411406270f1ee_hires.jpeg',\n",
    "        './datasets/ab_biggan/2012fae76a825b03405d8f46_hires.jpeg'\n",
    "    ],\n",
    "    [\n",
    "#         './datasets/ab_biggan/2db513d411406270f1ee_hires.jpeg',\n",
    "        './datasets/ab_biggan/444d1f1c0ce6e5842c61_hires.jpeg',\n",
    "        './datasets/ab_biggan/60009772d97f8a8fa0fe_hires.jpeg',\n",
    "        \n",
    "#         './datasets/ab_biggan/a81e9cae83db1804b094_hires.jpeg'\n",
    "    ],\n",
    "    [\n",
    "        './datasets/sci-bio-art/7c78e087c6fd92a0e884_hires.jpeg',\n",
    "        './datasets/sci-bio-art/285cf1ce669edeab9041_hires.jpeg',\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "USE_BIGGAN = False\n",
    "if USE_BIGGAN:\n",
    "    n_refs = 24*2\n",
    "    biggan = BigGAN.from_pretrained(f'biggan-deep-{img_size}').cuda()\n",
    "    palette = random_biggan(n_refs, img_size, biggan, seed=1, truncation=.4)\n",
    "else:\n",
    "#     palette_imgs_large = torch.cat([\n",
    "#        load_directory('./datasets/eyes_closed/', 1024),\n",
    "#        load_directory('./datasets/artbreeder/', img_size)\n",
    "#     ])\n",
    "    img_names = img_sets[2]\n",
    "    palette_imgs_large = load_images(img_names, 1024)\n",
    "    palette_imgs = F.interpolate(palette_imgs_large, size=(img_size, img_size), mode='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_per_img = 20 // palette_imgs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_tensors(palette_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collager = Collager(palette_imgs, mask_generator, img_size, patch_per_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = [\n",
    "        collager(*collager.makeRandom(seed=i, trans_scale=.2))[0]\n",
    "        for i in range(2)\n",
    "    ] \n",
    "    draw_tensors(torch.stack(imgs).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_steps=600\n",
    "seed=None\n",
    "lr=1e-2\n",
    "frames = []\n",
    "#save_every_step = False\n",
    "if seed is not None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "collage_data = collager.makeRandom(seed=seed, trans_scale=.2)\n",
    "params = collage_data\n",
    "Z = collage_data[0]\n",
    "for x in collage_data:\n",
    "    x.requires_grad_(True)\n",
    "opt = torch.optim.Adam(params, lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=lr, total_steps=n_steps)\n",
    "pbar = tqdm(total=n_steps)\n",
    "loss_history = []\n",
    "\n",
    "for i in range(n_steps):\n",
    "    percent = i / n_steps\n",
    "    pbar.update()        \n",
    "    opt.zero_grad()\n",
    "    fl = torch.zeros(1)\n",
    "    norm_loss = .25 * Z.norm()\n",
    "    img, data = collager(*collage_data, return_data=False)\n",
    "    aug = augment(img, n=3)\n",
    "    fl = face_loss(((aug+1)*.5)).mean()\n",
    "    loss = fl + norm_loss - .001*img.mean()\n",
    "    loss_history.append(loss.detach().cpu().item())\n",
    "    loss.backward(retain_graph=True)\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    #if save_every_step:\n",
    "    #    data = export_collager(*collage_data, return_data=True)\n",
    "    #    saver.save(*data, final=True)\n",
    "    pbar.set_description(f\"fl: {fl.item():.3f}\")\n",
    "    frames.append(np.array(convert_to_images(img.detach().cpu())[0]))\n",
    "    if i % 50 == 0 and i > 0 or i == n_steps-1:\n",
    "        draw_tensors(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_img, opt_collage_data, opt_history = \\\n",
    "    img.detach(), tuple(x.detach() for x in collage_data), loss_history\n",
    "_= plt.plot(opt_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export video, highres image and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = CollageSaver()\n",
    "saver.save_palette(palette_imgs)\n",
    "print(saver.path)\n",
    "saver.save_video(frames)\n",
    "\n",
    "# Regenerate at 2x scale.\n",
    "export_collager = Collager(palette_imgs_large, mask_generator, 1024, patch_per_img)\n",
    "with torch.no_grad():\n",
    "    hires, data = export_collager(*collage_data, return_data=True) \n",
    "    saver.save(hires, data, final=True)\n",
    "\n",
    "if img_names:\n",
    "    with open(saver.path / 'image_names.txt', 'w') as outfile:\n",
    "        json.dump(img_names, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
